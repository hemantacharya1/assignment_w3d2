# ğŸ§  LLM Comparison CLI

This is a command-line tool that allows users to compare **Base**, **Instruct**, and **Fine-tuned** models from different providers using **locally running LLMs via Ollama**.

## ğŸ“¦ Features

- Compare outputs of models like `llama2`, `mistral:instruct`, and `gemma:2`
- View model metadata: parameter size, instruction-following ability, context window, etc.
- See actual token usage using Hugging Face tokenizers
- Run fully locally (no OpenAI/Anthropic API keys required)
- Easily extendable

---

## ğŸš€ Setup Instructions

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install transformers sentencepiece python-dotenv
```

### 2. Install & run Ollama models

Make sure you have Ollama installed and running.

```bash
ollama run llama2
ollama run mistral:instruct
ollama run gemma:2
```

### 3. Create .env file (optional if using gated models)

```env
HF_TOKEN=your_huggingface_token_here  # only needed if accessing gated models
```

### 4. Run the CLI tool

```bash
python main.py --prompt "Explain blackhole to a 5yr old"
```

## ğŸ“ Project Structure

```
llm-comparator/
â”œâ”€â”€ main.py                  # CLI entry point
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ollama_runner.py     # Runs local LLMs via Ollama
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ token_counter.py     # Counts tokens using HF tokenizers
â”œâ”€â”€ comparisons.md           # Summary table of 5 prompts (your task)
â”œâ”€â”€ .env.example             # Sample env file
â””â”€â”€ README.md
```

## âœ… Supported Models

| Model | Type | Parameters | Context Size | Instruction Following |
|-------|------|------------|--------------|----------------------|
| llama2 | Base | 7B | 4096 tokens | âŒ No |
| mistral:instruct | Instruct | 7B | 8192 tokens | âœ… Yes |
| gemma:2 | Fine-tuned | 2B | 8192 tokens | âœ… Partial |

## ğŸ”“ License

MIT License

---